ca#################################################################
#								#
#	Créateur : Arthur YANG - Responsable Documentation 	#
#				 & Administratif		#
			
#	Date de création : 18/02/2025				#
#								#
#	Dernier modificateur : Arthur YANG			#
#	Date de modification : 16/03/2025			#
#								#
#		Version actuelle : 1.0				#
#								#
#################################################################


----------------------- Configuration et sécurisation du serveur RDS (test) ----------------------- 

Sources : 


Actions effectuées dans le conteneur : 

   => Connexion via root / mot de passe
   => apt install unzip
   => Installation de l'agent promtail
   => Création d'un user "test" et son groupe sans accès interactif. (useradd --system --no-create-home --home /nonexistent --shell /usr/sbin/nologin test)

	Installation de Promtail
  		 => Téléchargement et installation :
		On se place dans le chemin : /usr/local/bin/
		On télécharge le fichier zip de Promtail : wget https://github.com/grafana/loki/releases/download/v3.4.2/promtail-linux-amd64.zip
		Nous pouvons ensuite l'unzip : unzip (nom du fichier)
		C'est un fichier executable que l'on a.

	=> Configuration de Promtail :

	Nous devons ici, avoir un fichier nommé "promtail-config.yaml" pour la configuration de Promtail.
	Nous pouvons télécharger un modèle pré-existant : wget https://github.com/grafana/loki/blob/main/clients/cmd/promtail/promtail-local-config.yaml ou le créer nous même.
	Cela télécharge un modèle que l'on peut renommer comme nous le voulions.


=> Config .yaml personnalisé que nous effectuons :

--------------------------------------------------------------------------------------------

server:
  http_listen_port: 9080
  grpc_listen_port: 0

positions:
  filename: /var/lib/promtail/positions.yaml

clients:
  - url: http://192.168.7.133:3100/loki/api/v1/push  # Utilisation de l'IP du serveur Loki

scrape_configs:
  - job_name: SRV-RDS-AARD-SYSLOG
    static_configs:
      - targets:
          - localhost
        labels:
          job: SYSLOG-AARD-RDS
          host: SRV-RDS-AARD
          stream: stdout
          __path__: /var/log/syslog

  - job_name: SRV-RDS-AARD-MAIL
    static_configs:
      - targets:
          - localhost
        labels:
          job: MAIL-AARD-RDS
          host: SRV-RDS-AARD
          stream: stdout
          __path__: /var/log/mail.log

  - job_name: SRV-RDS-AARD-CRON
    static_configs:
      - targets:
          - localhost
        labels:
          job: CRON-AARD-RDS
          host: SRV-RDS-AARD
          stream: stdout
          __path__: /var/log/cron.log

  - job_name: SRV-RDS-AARD-AUTHLOG
    static_configs:
      - targets:
          - localhost
        labels:
          job: AUTHLOG-AARD-RDS
          host: SRV-RDS-AARD
          stream: stdout
          __path__: /var/log/auth.log

  - job_name: SRV-RDS-AARD-USERLOG
    static_configs:
      - targets:
          - localhost
        labels:
          job: USERLOG-AARD-RDS
          host: SRV-RDS-AARD
          stream: stdout
          __path__: /var/log/user.log

  - job_name: SRV-RDS-AARD-XRDPLOG
    static_configs:
      - targets:
          - localhost
        labels:
          job: XRDPLOG-AARD-RDS
          host: SRV-RDS-AARD
          stream: stdout
          __path__: /var/log/xrdp.log

--------------------------------------------------------------------------------------------

Pour autoriser promtail à récuperer le xdrp.log, on fait : chmod 770 /var/log/xrdp.log


=> Créer un dossier /var/lib/promtail/ : mkdir -p /var/lib/promtail
		=> chown -R test:test /var/lib/promtail
		=> chmod -R 750 /var/lib/promtail

	=> Nous mettons l'utilisateur créée plus tôt "test" en tant que propriétaire et propriétaire groupe des fichiers de config de Loki.
	
		=> chown -R test:test promtail-config.yaml promtail-linux-amd64
		=> chmod 755 promtail-linux-amd64
		=> chmod 640 promtail-config.yaml


	=> Création du fichier : /etc/systemd/system/promtail.service


Pour ce serveur qui a un problème de CPU et RAM avec promtail, nous limitions le service afin qu'il ne consomme pas trop.

	=> Limite mémoire à 50 Mo
	=> Limite CPU à 10%
	=> Processus avec priorité basse (Nice=10)

--------------------------------------------------------------------------------------------

[Unit]
Description=Promtail Loki
Wants=network-online.target
After=network-online.target

[Service]
Type=simple
User=test
Group=test
MemoryLimit=50M
CPUQuota=10%
Nice=10
ExecStart=/usr/local/bin/promtail-linux-amd64 -config.file=/usr/local/bin/promtail-config.yaml

SyslogIdentifier=test
Restart=always

[Install]
WantedBy=multi-user.target

--------------------------------------------------------------------------------------------


Commandes système et visualisation des logs en direct :

		=> systemctl enable promtail.service 
		=> systemctl start promtail.service
		=> systemctl status promtail.service
		=> journalctl -f -u promtail.service

	Normalement le service promtail est lancé.


Dans nos conteneurs LXC Debian, les logs semblent être dans le "journald" et non le syslog.

On installe "rsyslog" pour qu'ils puissent etre visible dans "syslog"

	=> apt install rsyslog -y
	=> systemctl enable rsyslog --now
	=> tail -f /var/log/syslog

Pour que notre utilisateur "test" puissent avoir les droits de lecture de logs, on va lui ajouter dans le groupe adm : 

	=> usermod -aG adm test
	=> systemctl restart promtail.service

En effet, Un problèmes d’accès aux fichiers /var/log/auth.log, /var/log/cron.log et /var/log/user.log par Grafana sera présent par le fait que l'on a paramétré le service promtail avec l'utilisateur "test".

Faire de même avec le fichier "syslog" : 

	=> chown test:root /var/log/syslog
	=> chmod 640 /var/log/syslog
	=> systemctl restart promtail.service

On peut rajouter : 

	=> chown test:utmp /var/log/btmp



De manière générale, regarder les permissions des fichiers selon l'utilisateur qui utilise le service.


Pour continuer la configuration, on va changer le nom du serveur, dans la ligne de config de promtail : 

	=> host: SRV-test-AARD > host: SRV-RDS-AARD
Nous faisons cela, car nous avons changé sur Proxmox et sur le conteneur directement son nom : SRV-TEST-AARD => SRV-RDS-AARD
Ca nous permettra sur GRAFANA de reconnaitre le bon serveur avec le bon nom à jour.

Nous avons changé le nom du containeur dans le fichier /etc/hostname et /etc/hosts

Utilisation d'un compte FIREFOX MOZILLA (DISPO DANS LE BITWARDEN) pour avoir pour tous, les raccourcies et marques pages de "base".
Compte générique exprès pour l'utilisation de ce serveur RDS.


	=> Etant donné que nous sommes en format maquette, et que je n'ai pas envie de créer a chaque fois plein d'adresse mail exprès, on va réutiliser la boite mail OUTLOOK dispo dans BW, "wizardsndice@outlook.com".
	=> Pour le mot de passe, je génère un nouveau mot de passe différent de la boite, dispo dans les "notes" de l'élément de la boite sur BW.

Dans les paramétrages, je choisi de tout synchro sauf les pages ouvertes, pour éviter de foutre le bordel dans le boulot des autres.

Pour l'aspect sécurité, je créé une clé de récup de mot de passe, dispo dans le BW.

Voila, maintenant je suis entrain de mettre les raccourcies utiles pour accéder aux différentes ressources de l'infrastructure. Il faudra que tout le monde se connecte avec le compte, pour pouvoir avoir accès au raccourcies.
Si un problème survient dans le futur, voir a utiliser Enterprise policy de firefox dans la conf JSON. Mais plus chiant a configurer.

Dans le fichier de configuration /etc/xrdp/xrdp.ini et dans la partie "LOGGIN"
	=> on enlève le # devant le SyslogLevel.

Ici Xrdp envoie ses logs vers le syslog mais aussi dans le fichier xrdp.log et xrdp-sesman.log dans /var/log .

Je vais laisser comme c'est actuellement car cela me parait suffisant pour un format maquette, néanmoins si besoin, nous pourrons toujours plus préciser pour obtenir ces fichiers séparemment et dans un répertoire plus spécifiques.

Voila pour la configuration basique du RDS. A vous de voir si vous voulez ajoutez quelque chose de plus.



On va installer pour la fin, NODE EXPORTER pour récupérer des informations plus précises que avec SNMP, qui surveillera les serveurs directs et non les équipements réseaux.

	=> Dans /usr/local/bin/ , on télécharge NODE Exporter qui va nous servir à récupérer les métriques du serveur RDS ici.

		=> wget https://github.com/prometheus/node_exporter/releases/download/v1.9.0/node_exporter-1.9.0.linux-amd64.tar.gz
		=> tar -xvzf node_exporter-1.9.0.linux-amd64.tar.gz
		=> rm node_exporter-1.9.0.linux-amd64.tar.gz
		=> mv /usr/local/bin/node_exporter-1.9.0.linux-amd64/node_exporter /usr/local/bin/
		=> chown -R test:test /usr/local/bin/node_exporter
		=> chmod 755 /usr/local/bin/node_exporter
		=> rm -r /usr/local/bin/node_exporter-1.9.0.linux-amd64/


=> Création du fichier : nano /etc/systemd/system/node_exporter.service

--------------------------------------------------------------------------------------------

[Unit]
Description=Prometheus NODE Exporter
Wants=network-online.target
After=network-online.target

[Service]
Type=simple
User=test
Group=test
ExecStart=/usr/local/bin/node_exporter --web.listen-address=0.0.0.0:XXXX --collector.systemd --collector.processes

SyslogIdentifier=test
Restart=always

[Install]
WantedBy=multi-user.target

--------------------------------------------------------------------------------------------

On a changé ici le port d'écoute de SNMP EXPORTER de Prometheus ici, dans le fichier du service. Ca ne se fait pas dans le .yml car il n'y pas de fichier .yml pour NODE Exporter .

Commandes système et visualisation des logs en direct :

		=> systemctl daemon-reexec
		=> systemctl daemon-reload
		=> systemctl enable node_exporter.service 
		=> systemctl start node_exporter.service 
		=> systemctl status node_exporter.service 
		=> journalctl -f -u node_exporter.service

Voila pour l'installation de NODE EXPORTER.


=> Ajout du MOTD personnalisé sur la VM (et toutes) du NAS.
/etc/motd

Wizards & Dice | Shell Access for Debian GNU/Linux

***********************************************************************************************************************
*							LEGAL NOTICE
* This system is for authorized use only. All activities on this system may be * monitored and recorded. Unauthorized access or use is strictly prohibited * and may result in disciplinary action, criminal prosecution, or both. By * continuing to use and access this system, you consent to such monitoring.*

***********************************************************************************************************************


=> Ajout des éléments pour le monitoring des machines et de leurs MAJS.

	=> wget https://raw.githubusercontent.com/labmonkeys-space/apt-prometheus/main/script/apt-metrics.sh -O /usr/local/bin/apt-metrics.sh
	=> chmod 750 /usr/local/bin/apt-metrics.sh
	=> chown test /usr/local/bin/apt-metrics.sh
	=> Puis on change tout le fichier : 

-------------------------------------------------------------------------------------------------------------------------

#!/bin/bash

OUT_FILE="/var/lib/node_exporter/textfile_collector/apt_updates.prom"

# Total updates
TOTAL_UPDATES=$(apt list --upgradeable 2>/dev/null | grep -v "Listing" | wc -l)

# Security updates (si 'unattended-upgrades' est installé)
SEC_UPDATES=$(apt list --upgradeable 2>/dev/null | grep security | wc -l)

# Reboot required (flag fichier)
if [ -f /var/run/reboot-required ]; then
  REBOOT_NEEDED=1
else
  REBOOT_NEEDED=0
fi

# Export en format Prometheus
echo "# HELP debian_apt_updates Nombre total de mises à jour APT" > "$OUT_FILE"
echo "# TYPE debian_apt_updates gauge" >> "$OUT_FILE"
echo "debian_apt_updates $TOTAL_UPDATES" >> "$OUT_FILE"

echo "# HELP debian_security_updates Nombre de mises à jour de sécurité" >> "$OUT_FILE"
echo "# TYPE debian_security_updates gauge" >> "$OUT_FILE"
echo "debian_security_updates $SEC_UPDATES" >> "$OUT_FILE"

echo "# HELP debian_reboot_required Système nécessite un redémarrage" >> "$OUT_FILE"
echo "# TYPE debian_reboot_required gauge" >> "$OUT_FILE"
echo "debian_reboot_required $REBOOT_NEEDED" >> "$OUT_FILE"

-------------------------------------------------------------------------------------------------------------------------

Dans le node_exporter.service

-------------------------------------------------------------------------------------------------------------------------

[Unit]
Description=Prometheus NODE Exporter
Wants=network-online.target
After=network-online.target

[Service]
Type=simple
User=test
Group=test
ExecStart=/usr/local/bin/node_exporter --web.listen-address=0.0.0.0:XXXX --collector.systemd --collector.processes --collector.textfile --collector.textfile.directory=/var/lib/node_exporter/textfile_collector

SyslogIdentifier=rproxy
Restart=always

[Install]
WantedBy=multi-user.target

-------------------------------------------------------------------------------------------------------------------------

=> mkdir -p /var/lib/node_exporter/textfile_collector
=> chown -R test:root /var/lib/node_exporter/textfile_collector
=> chmod -R 770 /var/lib/node_exporter/textfile_collector

		=> systemctl daemon-reexec
		=> systemctl daemon-reload
		=> systemctl restart node_exporter.service 
		=> systemctl status node_exporter.service

Il faut maintenant faire un cron, pour executer le script "apt-metrics.sh" régulièrement : 

-------------------------------------------------------------------------------------------------------------------------
=> crontab -e

30 7 * * * /usr/local/bin/apt-metrics.sh

=> systemctl restart cron.service

-------------------------------------------------------------------------------------------------------------------------

Maintenant il suffit sur grafana avec le dashboard, de regarder ce que nous voulons voir.

Pas de mise en place de UFW : 

	=> Au vu du temps, et la quantité de ports (je ne sais pas tout les ports non que ce soit en entrée ou sortie surtout), je ne met pas ici de UFW pour le NAS.



Installation Apparmor : 

=> apt install apparmor apparmor-utils

Vérifier que AppArmor est actif

=> aa-status

Activer AppArmor au démarrage

=> systemctl enable apparmor

Je laisse paramétrer automatiquement apparmor, car comme dit en bas : 

Je ne ferai pas plus de choses, en soit apparmor est installé mais il faudrait précisément pour certains services, donc faire des profiles minutieux.
Sauf que => Pas le temps.


Installation lynis + fail2ban

=> Executer lynis dans syslog : lynis audit system | logger -t lynis

OU

=> Executer le script dans /usr/local/bin/lynis-syslog.sh

nano /usr/local/bin/lynis-syslog.sh

Script : 

--------------------------------------

#!/bin/bash
lynis audit system | logger -t lynis

--------------------------------------

=> chmod 750 /usr/local/bin/lynis-syslog.sh
=> chown test /usr/local/bin/lynis-syslog.sh

Pas de cron.

On peut voir depuis GRAFANA dans les logs, le résultat.

Fail2Ban : 

=> apt install fail2ban -y
=> systemctl enable fail2ban
=> systemctl start fail2ban

On ne modifie jamais directement jail.conf. On crée un fichier jail.local :

=> cp /etc/fail2ban/jail.conf /etc/fail2ban/jail.local
=> nano /etc/fail2ban/jail.local

--------------------------

[sshd]
enabled = true
port    = ssh
logpath = %(sshd_log)s
backend = %(sshd_backend)s
maxretry = 5
bantime = 500
findtime = 600

--------------------------

=> systemctl restart fail2ban
=> fail2ban-client status
=> fail2ban-client status sshd


Installation Clamav : 

=> apt install clamav -y
=> systemctl stop clamav-freshclam
=> freshclam
=> clamscan -r /home | logger -t clamav (pas obligatoire)